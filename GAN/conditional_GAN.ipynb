{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('AGG')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\") \n",
    "\n",
    "# display plots in this notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "Z_dim = 100\n",
    "h_dim = 128\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, X_dim], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=[None, y_dim], name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('D') as scope:\n",
    "    D_W1 = tf.get_variable('W1', shape=[X_dim + y_dim, h_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    D_b1 = tf.get_variable('b1', shape=[h_dim], initializer=tf.zeros_initializer())\n",
    "    D_W2 = tf.get_variable('W2', shape=[h_dim, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    D_b2 = tf.get_variable('b2', shape=[1], initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_D = [D_W1, D_b1, D_W2, D_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z = tf.placeholder(tf.float32, shape=[None, Z_dim], name='Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('G') as scope:\n",
    "    G_W1 = tf.get_variable('W1', shape=[Z_dim + y_dim, h_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    G_b1 = tf.get_variable('b1', shape=[h_dim], initializer=tf.zeros_initializer())\n",
    "    G_W2 = tf.get_variable('W2', shape=[h_dim, X_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    G_b2 = tf.get_variable('b2', shape=[X_dim], initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_G = [G_W1, G_b1, G_W2, G_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1, 1, size=[m, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, y):\n",
    "    inputs = tf.concat([z, y], axis=1)\n",
    "    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
    "    G_logit = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_logit)\n",
    "    \n",
    "    return G_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x, y):\n",
    "    inputs = tf.concat([x, y], axis=1)\n",
    "    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "    \n",
    "    return D_prob, D_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sample = generator(Z, y)\n",
    "D_prob_real, D_logit_real = discriminator(X, y)\n",
    "D_prob_fake, D_logit_fake = discriminator(G_sample, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "\n",
    "D_loss = D_loss_real + D_loss_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=var_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=var_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "D loss: 1.095\n",
      "G loss: 2.727\n",
      "Iter: 1000\n",
      "D loss: 0.006337\n",
      "G loss: 6.315\n",
      "Iter: 2000\n",
      "D loss: 0.02544\n",
      "G loss: 5.397\n",
      "Iter: 3000\n",
      "D loss: 0.05623\n",
      "G loss: 5.8\n",
      "Iter: 4000\n",
      "D loss: 0.1127\n",
      "G loss: 6.403\n",
      "Iter: 5000\n",
      "D loss: 0.2415\n",
      "G loss: 5.245\n",
      "Iter: 6000\n",
      "D loss: 0.4015\n",
      "G loss: 4.342\n",
      "Iter: 7000\n",
      "D loss: 0.4276\n",
      "G loss: 3.992\n",
      "Iter: 8000\n",
      "D loss: 0.4444\n",
      "G loss: 4.362\n",
      "Iter: 9000\n",
      "D loss: 0.4403\n",
      "G loss: 3.127\n",
      "Iter: 10000\n",
      "D loss: 0.7815\n",
      "G loss: 2.72\n",
      "Iter: 11000\n",
      "D loss: 0.7691\n",
      "G loss: 2.731\n",
      "Iter: 12000\n",
      "D loss: 0.6258\n",
      "G loss: 3.285\n",
      "Iter: 13000\n",
      "D loss: 0.6803\n",
      "G loss: 2.76\n",
      "Iter: 14000\n",
      "D loss: 0.8504\n",
      "G loss: 2.438\n",
      "Iter: 15000\n",
      "D loss: 1.085\n",
      "G loss: 2.111\n",
      "Iter: 16000\n",
      "D loss: 0.7681\n",
      "G loss: 2.009\n",
      "Iter: 17000\n",
      "D loss: 0.9182\n",
      "G loss: 1.827\n",
      "Iter: 18000\n",
      "D loss: 0.6815\n",
      "G loss: 1.982\n",
      "Iter: 19000\n",
      "D loss: 0.8129\n",
      "G loss: 2.507\n",
      "Iter: 20000\n",
      "D loss: 0.8879\n",
      "G loss: 2.148\n",
      "Iter: 21000\n",
      "D loss: 0.8758\n",
      "G loss: 1.598\n",
      "Iter: 22000\n",
      "D loss: 0.8852\n",
      "G loss: 1.815\n",
      "Iter: 23000\n",
      "D loss: 0.8628\n",
      "G loss: 1.933\n",
      "Iter: 24000\n",
      "D loss: 1.006\n",
      "G loss: 1.453\n",
      "Iter: 25000\n",
      "D loss: 0.9068\n",
      "G loss: 1.607\n",
      "Iter: 26000\n",
      "D loss: 0.6942\n",
      "G loss: 2.183\n",
      "Iter: 27000\n",
      "D loss: 0.8613\n",
      "G loss: 1.689\n",
      "Iter: 28000\n",
      "D loss: 0.8149\n",
      "G loss: 1.863\n",
      "Iter: 29000\n",
      "D loss: 0.909\n",
      "G loss: 1.667\n",
      "Iter: 30000\n",
      "D loss: 0.8847\n",
      "G loss: 1.78\n",
      "Iter: 31000\n",
      "D loss: 0.8311\n",
      "G loss: 1.792\n",
      "Iter: 32000\n",
      "D loss: 0.9139\n",
      "G loss: 1.828\n",
      "Iter: 33000\n",
      "D loss: 0.9359\n",
      "G loss: 1.919\n",
      "Iter: 34000\n",
      "D loss: 0.9333\n",
      "G loss: 1.918\n",
      "Iter: 35000\n",
      "D loss: 0.8933\n",
      "G loss: 1.706\n",
      "Iter: 36000\n",
      "D loss: 0.8219\n",
      "G loss: 1.971\n",
      "Iter: 37000\n",
      "D loss: 0.6975\n",
      "G loss: 1.973\n",
      "Iter: 38000\n",
      "D loss: 0.8109\n",
      "G loss: 1.39\n",
      "Iter: 39000\n",
      "D loss: 0.7677\n",
      "G loss: 1.991\n",
      "Iter: 40000\n",
      "D loss: 0.8722\n",
      "G loss: 1.494\n",
      "Iter: 41000\n",
      "D loss: 1.09\n",
      "G loss: 1.583\n",
      "Iter: 42000\n",
      "D loss: 0.7387\n",
      "G loss: 1.308\n",
      "Iter: 43000\n",
      "D loss: 0.933\n",
      "G loss: 1.517\n",
      "Iter: 44000\n",
      "D loss: 0.8346\n",
      "G loss: 1.592\n",
      "Iter: 45000\n",
      "D loss: 0.9469\n",
      "G loss: 1.838\n",
      "Iter: 46000\n",
      "D loss: 0.9404\n",
      "G loss: 1.66\n",
      "Iter: 47000\n",
      "D loss: 0.9159\n",
      "G loss: 1.348\n",
      "Iter: 48000\n",
      "D loss: 0.885\n",
      "G loss: 1.644\n",
      "Iter: 49000\n",
      "D loss: 0.7231\n",
      "G loss: 1.893\n",
      "Iter: 50000\n",
      "D loss: 0.9267\n",
      "G loss: 1.472\n",
      "Iter: 51000\n",
      "D loss: 0.9105\n",
      "G loss: 1.818\n",
      "Iter: 52000\n",
      "D loss: 0.8171\n",
      "G loss: 1.839\n",
      "Iter: 53000\n",
      "D loss: 0.7975\n",
      "G loss: 1.802\n",
      "Iter: 54000\n",
      "D loss: 0.8972\n",
      "G loss: 1.766\n",
      "Iter: 55000\n",
      "D loss: 0.9049\n",
      "G loss: 1.624\n",
      "Iter: 56000\n",
      "D loss: 0.9685\n",
      "G loss: 1.513\n",
      "Iter: 57000\n",
      "D loss: 1.03\n",
      "G loss: 1.559\n",
      "Iter: 58000\n",
      "D loss: 0.7959\n",
      "G loss: 1.594\n",
      "Iter: 59000\n",
      "D loss: 0.823\n",
      "G loss: 1.481\n",
      "Iter: 60000\n",
      "D loss: 0.8212\n",
      "G loss: 1.7\n",
      "Iter: 61000\n",
      "D loss: 0.8904\n",
      "G loss: 1.64\n",
      "Iter: 62000\n",
      "D loss: 0.9725\n",
      "G loss: 1.531\n",
      "Iter: 63000\n",
      "D loss: 0.849\n",
      "G loss: 1.93\n",
      "Iter: 64000\n",
      "D loss: 0.961\n",
      "G loss: 1.544\n",
      "Iter: 65000\n",
      "D loss: 0.7803\n",
      "G loss: 1.707\n",
      "Iter: 66000\n",
      "D loss: 0.7745\n",
      "G loss: 1.667\n",
      "Iter: 67000\n",
      "D loss: 0.782\n",
      "G loss: 1.592\n",
      "Iter: 68000\n",
      "D loss: 1.02\n",
      "G loss: 1.53\n",
      "Iter: 69000\n",
      "D loss: 0.8374\n",
      "G loss: 1.537\n",
      "Iter: 70000\n",
      "D loss: 0.9236\n",
      "G loss: 1.786\n",
      "Iter: 71000\n",
      "D loss: 0.8807\n",
      "G loss: 1.647\n",
      "Iter: 72000\n",
      "D loss: 0.9234\n",
      "G loss: 1.708\n",
      "Iter: 73000\n",
      "D loss: 0.9\n",
      "G loss: 1.645\n",
      "Iter: 74000\n",
      "D loss: 0.8129\n",
      "G loss: 1.823\n",
      "Iter: 75000\n",
      "D loss: 0.7823\n",
      "G loss: 1.723\n",
      "Iter: 76000\n",
      "D loss: 0.787\n",
      "G loss: 1.877\n",
      "Iter: 77000\n",
      "D loss: 0.83\n",
      "G loss: 1.685\n",
      "Iter: 78000\n",
      "D loss: 0.733\n",
      "G loss: 1.679\n",
      "Iter: 79000\n",
      "D loss: 0.9004\n",
      "G loss: 1.792\n",
      "Iter: 80000\n",
      "D loss: 0.7914\n",
      "G loss: 1.75\n",
      "Iter: 81000\n",
      "D loss: 0.7733\n",
      "G loss: 1.471\n",
      "Iter: 82000\n",
      "D loss: 0.8531\n",
      "G loss: 1.511\n",
      "Iter: 83000\n",
      "D loss: 0.7502\n",
      "G loss: 1.968\n",
      "Iter: 84000\n",
      "D loss: 0.8562\n",
      "G loss: 1.655\n",
      "Iter: 85000\n",
      "D loss: 0.8463\n",
      "G loss: 2.0\n",
      "Iter: 86000\n",
      "D loss: 0.8256\n",
      "G loss: 1.776\n",
      "Iter: 87000\n",
      "D loss: 0.803\n",
      "G loss: 1.795\n",
      "Iter: 88000\n",
      "D loss: 0.7918\n",
      "G loss: 1.743\n",
      "Iter: 89000\n",
      "D loss: 0.8568\n",
      "G loss: 1.761\n",
      "Iter: 90000\n",
      "D loss: 0.7729\n",
      "G loss: 1.775\n",
      "Iter: 91000\n",
      "D loss: 0.8185\n",
      "G loss: 1.773\n",
      "Iter: 92000\n",
      "D loss: 0.7185\n",
      "G loss: 1.702\n",
      "Iter: 93000\n",
      "D loss: 0.7645\n",
      "G loss: 1.643\n",
      "Iter: 94000\n",
      "D loss: 0.8873\n",
      "G loss: 1.755\n",
      "Iter: 95000\n",
      "D loss: 0.7978\n",
      "G loss: 1.901\n",
      "Iter: 96000\n",
      "D loss: 0.7978\n",
      "G loss: 2.161\n",
      "Iter: 97000\n",
      "D loss: 0.8368\n",
      "G loss: 1.712\n",
      "Iter: 98000\n",
      "D loss: 0.9777\n",
      "G loss: 1.701\n",
      "Iter: 99000\n",
      "D loss: 0.7514\n",
      "G loss: 1.564\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "samples_dict = {}\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(100000):\n",
    "        if i % 1000 == 0:\n",
    "            n_samples = 16\n",
    "            y_sample = np.zeros(shape=[n_samples, y_dim])\n",
    "            for k in range(n_samples):\n",
    "                y_sample[k, np.random.randint(0, y_dim)] = 1\n",
    "            samples = sess.run(G_sample, feed_dict={y: y_sample, Z: sample_Z(n_samples, Z_dim)})\n",
    "            samples_dict[i] = samples\n",
    "        \n",
    "        X_mb, y_mb = mnist.train.next_batch(batch_size)\n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, y: y_mb, Z: sample_Z(batch_size, Z_dim)})\n",
    "        _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={y: y_mb, Z: sample_Z(batch_size, Z_dim)})\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print \"Iter: {}\".format(i)\n",
    "            print \"D loss: {:.4}\".format(D_loss_curr)\n",
    "            print \"G loss: {:.4}\".format(G_loss_curr)\n",
    "            \n",
    "            saver.save(sess, 'save/conditional_GAN/conditional_GAN', global_step=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/conditional_GAN/conditional_GAN-99000\n",
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPYAAAD0CAYAAACo2tvDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1121c2110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('save/conditional_GAN'))\n",
    "    \n",
    "    n_samples = 1\n",
    "    y_sample = np.zeros(shape=[n_samples, y_dim])\n",
    "    for k in range(n_samples):\n",
    "        digit = np.random.randint(0, y_dim)\n",
    "        y_sample[k, digit] = 1\n",
    "    samples = sess.run(G_sample, feed_dict={y: y_sample, Z: sample_Z(n_samples, Z_dim)})\n",
    "    print digit\n",
    "    plt.imshow(samples[0,:].reshape((28,28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
